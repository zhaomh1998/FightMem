{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import pdfplumber\n",
    "import ebisu\n",
    "import pandas as pd\n",
    "pdf = pdfplumber.open(\"data/GRE1450.pdf\").pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row(row):\n",
    "    assert isinstance(row, list)\n",
    "    assert len(row) == 5\n",
    "    if row[0] == '例例句句': # 例句\n",
    "#         print(f\"例句{row[1]}\")\n",
    "        return True, row[1]\n",
    "    elif re.search(r'[\\u4e00-\\u9fa5]+', row[0]):  # 含有中文，扔掉\n",
    "        return None\n",
    "    elif row[0] == '' or re.search(r'^Day ', row[0]):\n",
    "        return None\n",
    "    else: # 单词\n",
    "        meaning = row[2].replace('\\u2028','')\n",
    "        meaning = [i.replace('\\n', '') for i in re.split(r'\\n(?=\\()', meaning)]\n",
    "        meaning = '\\n'.join(meaning)\n",
    "        # https://jrgraphix.net/research/unicode_blocks.php\n",
    "        meaning = re.sub(r'[\\u2f00-\\u2fdf]', '', meaning)   # Kangxi\n",
    "        meaning = re.sub(r'[\\u3400-\\u4dbf]', '', meaning)   # CJK Extras\n",
    "        meaning = re.sub(r'[\\uf900-\\ufaff]', '', meaning)   # CJK Extras\n",
    "#         meaning = row[2].replace('\\n', ' ')\n",
    "#         print(f\"单词{row[0]} 音标{row[1]} 释义{meaning} 同义词{row[3]}\")\n",
    "        return False, (row[0], row[1], meaning, row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading page 0...\n",
      "Reading page 1...\n",
      "Reading page 2...\n",
      "Reading page 3...\n",
      "Reading page 4...\n",
      "Reading page 5...\n",
      "Reading page 6...\n",
      "Reading page 7...\n",
      "Reading page 8...\n",
      "Reading page 9...\n",
      "Reading page 10...\n",
      "Reading page 11...\n",
      "Reading page 12...\n",
      "Reading page 13...\n",
      "Reading page 14...\n",
      "Reading page 15...\n",
      "Reading page 16...\n",
      "Reading page 17...\n",
      "Reading page 18...\n",
      "Reading page 19...\n",
      "Reading page 20...\n",
      "Reading page 21...\n",
      "Reading page 22...\n",
      "Reading page 23...\n",
      "Reading page 24...\n",
      "Reading page 25...\n",
      "Reading page 26...\n",
      "Reading page 27...\n",
      "Reading page 28...\n",
      "Reading page 29...\n",
      "Reading page 30...\n",
      "Reading page 31...\n",
      "Reading page 32...\n",
      "Reading page 33...\n",
      "Reading page 34...\n",
      "Reading page 35...\n",
      "Reading page 36...\n",
      "Reading page 37...\n",
      "Reading page 38...\n",
      "Reading page 39...\n",
      "Reading page 40...\n",
      "Reading page 41...\n",
      "Reading page 42...\n",
      "Reading page 43...\n",
      "Reading page 44...\n",
      "Reading page 45...\n",
      "Reading page 46...\n",
      "Reading page 47...\n",
      "Reading page 48...\n",
      "Reading page 49...\n",
      "Reading page 50...\n",
      "Reading page 51...\n",
      "Reading page 52...\n",
      "Reading page 53...\n",
      "Reading page 54...\n",
      "Reading page 55...\n",
      "Reading page 56...\n",
      "Reading page 57...\n",
      "Reading page 58...\n",
      "Reading page 59...\n",
      "Reading page 60...\n",
      "Reading page 61...\n",
      "Reading page 62...\n",
      "Reading page 63...\n",
      "Reading page 64...\n",
      "Reading page 65...\n",
      "Reading page 66...\n",
      "Reading page 67...\n",
      "Reading page 68...\n",
      "Reading page 69...\n",
      "Reading page 70...\n",
      "Reading page 71...\n",
      "Reading page 72...\n",
      "Reading page 73...\n",
      "Reading page 74...\n",
      "Reading page 75...\n",
      "Reading page 76...\n",
      "Reading page 77...\n",
      "Reading page 78...\n",
      "Reading page 79...\n",
      "Reading page 80...\n",
      "Reading page 81...\n",
      "Reading page 82...\n",
      "Reading page 83...\n",
      "Reading page 84...\n",
      "Reading page 85...\n",
      "Reading page 86...\n",
      "Reading page 87...\n",
      "Reading page 88...\n",
      "Reading page 89...\n",
      "Reading page 90...\n",
      "Reading page 91...\n",
      "Reading page 92...\n",
      "Reading page 93...\n",
      "Reading page 94...\n",
      "Reading page 95...\n",
      "Reading page 96...\n",
      "Reading page 97...\n",
      "Reading page 98...\n"
     ]
    }
   ],
   "source": [
    "all_rows = list()\n",
    "for i in range(len(pdf)):\n",
    "    print(f\"Reading page {i}...\")\n",
    "    parsed = pdf[i].extract_tables()\n",
    "    assert len(parsed) == 1\n",
    "    all_rows.extend(parsed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 0\n",
    "last_word = None\n",
    "\n",
    "word_row = {'word': [], 'pron': [], 'mean': [], 'syn': []}\n",
    "example = {'word': [], 'ex': []}\n",
    "\n",
    "for row in all_rows:\n",
    "    parsed = parse_row(row)\n",
    "    if parsed is not None:\n",
    "        is_example, data = parsed\n",
    "        if not is_example:\n",
    "            word, pron, mean, syn = data\n",
    "            last_word = word\n",
    "            word_row['word'].append(word)\n",
    "            word_row['pron'].append(pron)\n",
    "            word_row['mean'].append(mean)\n",
    "            word_row['syn'].append(syn)\n",
    "        else:\n",
    "            assert last_word is not None\n",
    "            example['word'].append(last_word)\n",
    "            example['ex'].append(parsed[1])\n",
    "\n",
    "data_df = pd.DataFrame(word_row)\n",
    "ex_df = pd.DataFrame(example)\n",
    "data_df = data_df.merge(ex_df, on='word', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(data_df, open('data/GRE1450.fmknowledge', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebisu\n",
    "\n",
    "defaultModel = (4., 4., 24.) # alpha, beta, and half-life in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "date0 = datetime(2017, 4, 19, 22, 0, 0)\n",
    "\n",
    "database = [dict(factID=1, model=defaultModel, lastTest=date0),\n",
    "            dict(factID=2, model=defaultModel, lastTest=date0 + timedelta(hours=11))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHour = timedelta(hours=1)\n",
    "\n",
    "now = date0 + timedelta(hours=11.1)\n",
    "print(\"On {},\".format(now))\n",
    "for row in database:\n",
    "    recall = ebisu.predictRecall(row['model'],\n",
    "                                 (now - row['lastTest']) / oneHour,\n",
    "                                 exact=True)\n",
    "    print(\"Fact #{} probability of recall: {:0.1f}%\".format(row['factID'], recall * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
